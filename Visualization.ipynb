{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPVdmraFvRV93XT8ala5ivb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AaryanS11010/DataVisualizationBoneFracture/blob/main/Visualization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lTixVzUybYh1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "from enum import Enum\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import models"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yrjTAHF5bjQt",
        "outputId": "221005ee-2dae-4f21-b103-19af824c6c29"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)), # Resizes the image to be a standard size\n",
        "    transforms.ToTensor(), # Converts the image to tensor\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], # Normalizes the image\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])"
      ],
      "metadata": {
        "id": "w4O95Lqvbn3d"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9994664",
        "outputId": "5198303a-c922-4334-e0ca-f55fbda0f22f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5903bf7",
        "outputId": "ac9ec292-7130-4a1c-ab0a-586dafd9ce31"
      },
      "source": [
        "!ls /content/"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = datasets.ImageFolder(root='/content/drive/MyDrive/colab/fracture_dislocation', transform=transform)\n",
        "val_dataset = datasets.ImageFolder(root='/content/drive/MyDrive/colab/fracture_dislocation', transform=transform)"
      ],
      "metadata": {
        "id": "mXVyJQcMbr-N"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)"
      ],
      "metadata": {
        "id": "9Rpsvd16vwUY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "7f3baa43-2f8e-4ce0-b584-f0903839be63"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = train_dataset.classes\n",
        "num_classes = class_names.__len__()\n",
        "\n",
        "label_enum = Enum('ClassEnum', {name: idx for idx, name in enumerate(class_names)})\n",
        "\n",
        "print(class_names)\n",
        "print(num_classes)"
      ],
      "metadata": {
        "id": "PtzIBNJ8vyxI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21f2d5f4-5411-404d-86c5-15c78e0843bd"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['train', 'val']\n",
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining our own CNN\n",
        "\n",
        "class BoneFractureModel(nn.Module):\n",
        "    def __init__(self, num_classes, input_size=(3, 224, 224)):\n",
        "        super(BoneFractureModel, self).__init__()\n",
        "\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2,2),\n",
        "\n",
        "            nn.Conv2d(32, 64, 3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2,2),\n",
        "\n",
        "            nn.Conv2d(64, 128, 3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2,2),\n",
        "        )\n",
        "\n",
        "        # Automatically compute flatten size based on input image\n",
        "        with torch.no_grad():\n",
        "            dummy_input = torch.zeros(1, *input_size)  # matches your actual image size\n",
        "            dummy_output = self.conv_layers(dummy_input)\n",
        "            flatten_size = dummy_output.view(1, -1).size(1)\n",
        "\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Linear(flatten_size, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_layers(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc_layers(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "iw2rwvfEv09q"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training and validation function\n",
        "\n",
        "def train_and_validate(model, train_loader, val_loader, num_epochs=10, learning_rate=0.001, device=None):\n",
        "    if device is None:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    # Loss function and optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # --- Training ---\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for images, labels in train_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        train_loss = running_loss / total\n",
        "        train_acc = correct / total\n",
        "\n",
        "        # --- Validation ---\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "        with torch.no_grad():\n",
        "            for images, labels in val_loader:\n",
        "                images = images.to(device)\n",
        "                labels = labels.to(device)\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                val_loss += loss.item() * images.size(0)\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                val_total += labels.size(0)\n",
        "                val_correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_loss /= val_total\n",
        "        val_acc = val_correct / val_total\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} — \"\n",
        "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} — \"\n",
        "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "    print(\"Training complete!\")"
      ],
      "metadata": {
        "id": "lJxMWd5hv5Nq"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize our model as our used defined BoneFractureModel\n",
        "model = BoneFractureModel(num_classes)\n",
        "\n",
        "# Optional: print our model\n",
        "print(model)\n",
        "\n",
        "# Move Model to the GPU if available\n",
        "model.to(device)\n",
        "\n",
        "# Train the Model\n",
        "train_and_validate(model, train_loader, val_loader, num_epochs=5, learning_rate=0.001)"
      ],
      "metadata": {
        "id": "4ql2zUrtv7zc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e192fad4-bcef-4984-b930-917b046d335d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BoneFractureModel(\n",
            "  (conv_layers): Sequential(\n",
            "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU()\n",
            "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (4): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (6): ReLU()\n",
            "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (8): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (9): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (10): ReLU()\n",
            "    (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (fc_layers): Sequential(\n",
            "    (0): Linear(in_features=100352, out_features=128, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Dropout(p=0.5, inplace=False)\n",
            "    (3): Linear(in_features=128, out_features=2, bias=True)\n",
            "  )\n",
            ")\n",
            "Epoch 1/5 — Train Loss: 10.9278, Train Acc: 0.7564 — Val Loss: 3.9268, Val Acc: 0.8782\n",
            "Epoch 2/5 — Train Loss: 6.5554, Train Acc: 0.7308 — Val Loss: 0.6972, Val Acc: 0.8782\n",
            "Epoch 3/5 — Train Loss: 2.9713, Train Acc: 0.8333 — Val Loss: 0.8795, Val Acc: 0.8782\n",
            "Epoch 4/5 — Train Loss: 1.2532, Train Acc: 0.8333 — Val Loss: 0.6430, Val Acc: 0.8846\n",
            "Epoch 5/5 — Train Loss: 0.9205, Train Acc: 0.8718 — Val Loss: 0.5139, Val Acc: 0.8846\n",
            "Training complete!\n"
          ]
        }
      ]
    }
  ]
}